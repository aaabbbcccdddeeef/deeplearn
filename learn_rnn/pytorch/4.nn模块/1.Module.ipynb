{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[-1.7110],\n        [-1.2377],\n        [-0.2291],\n        [-0.6955]])\ntensor([3.])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#实现一个全连接层\n",
    "import torch as t;\n",
    "import torch.nn as nn\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self,input_feature,out_feature):\n",
    "        nn.Module.__init__(self)\n",
    "        #nn.Prameter是自动算梯度的\n",
    "        self.w=nn.Parameter(t.randn(input_feature,out_feature))\n",
    "        self.b=nn.Parameter(t.randn(out_feature))\n",
    "    def forward(self,x):\n",
    "        return x.mm(self.w)+self.b\n",
    "    \n",
    "layer=Linear(4,1)\n",
    "rtn=layer(t.randn(3,4))\n",
    "rtn.backward(t.ones(rtn.size()))  # 计算梯度\n",
    "print(layer.w.grad)  # 获取w的梯度\n",
    "print(layer.b.grad)  # 获取b的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[  -12.4170],\n        [ -211.0885],\n        [ -314.3124],\n        [  234.5295],\n        [  -74.2657],\n        [ -413.3431],\n        [  135.3651],\n        [ -247.7416],\n        [ -210.7835],\n        [ -400.5256],\n        [ -425.7328],\n        [ -291.4088],\n        [ -765.7511],\n        [-1160.9725],\n        [ -447.3388],\n        [ -511.2890],\n        [  -25.5647],\n        [   16.1510],\n        [ -792.5206],\n        [ -403.8644],\n        [  290.3611],\n        [ -279.8174],\n        [ -362.0346],\n        [   70.0442],\n        [ -538.4562],\n        [ -255.3131],\n        [ -273.7527],\n        [ -227.2953],\n        [ -442.9148],\n        [  319.1536],\n        [   48.4505],\n        [   26.5859],\n        [ -738.0776],\n        [  236.8847],\n        [ -329.5198],\n        [    2.1988],\n        [ -397.8156],\n        [  -90.0620],\n        [    4.5624],\n        [  309.9224],\n        [ -487.7955],\n        [  -31.2471],\n        [  291.5693],\n        [  512.4156],\n        [ -292.4831],\n        [  -92.4021],\n        [  290.7582],\n        [ -490.4692],\n        [ -439.7261],\n        [  327.9742],\n        [ -409.4265],\n        [ -184.5044],\n        [  385.5742],\n        [   35.2770],\n        [ -176.3798],\n        [ -540.0377],\n        [  222.5312],\n        [ -364.2851],\n        [  290.0815],\n        [  -63.3543],\n        [ -260.7884],\n        [   37.5903],\n        [  139.1799],\n        [ -545.8848],\n        [  -52.9185],\n        [ -454.0047],\n        [  302.0286],\n        [ -190.1423],\n        [ -358.5752],\n        [  279.0520],\n        [   68.4848],\n        [  153.2764],\n        [  -46.3757],\n        [ -192.9146],\n        [ -311.3524],\n        [ -586.2933],\n        [ -391.5928],\n        [  197.4883],\n        [ -358.3928],\n        [  243.3159],\n        [  103.7775],\n        [ -330.0328],\n        [  -61.4577],\n        [ -718.8087],\n        [  -99.4366],\n        [-1005.7531],\n        [ -132.1186],\n        [ -577.9149],\n        [ -671.1042],\n        [ -192.0712],\n        [ -401.9549],\n        [ -331.3533],\n        [  426.7212],\n        [ -510.6834],\n        [ -365.5705],\n        [  -62.8509],\n        [ -762.1498],\n        [ -219.9713],\n        [   32.0608],\n        [ -343.9088],\n        [  407.0339],\n        [  377.7293],\n        [  316.2007],\n        [   67.9055],\n        [  380.3983],\n        [  -19.1209],\n        [  -13.7253],\n        [ -172.5935],\n        [  246.1086],\n        [  249.5518],\n        [ -463.4698],\n        [ -709.5825],\n        [ -718.4502],\n        [ -721.2395],\n        [ -308.7171],\n        [ -745.4994],\n        [ -242.9814],\n        [    3.7729],\n        [ -102.6302],\n        [ -743.5651],\n        [ -500.9555],\n        [  -42.0055],\n        [   25.6647],\n        [ -761.7082],\n        [  181.0730],\n        [ -504.5703],\n        [ -241.7809],\n        [  -33.3882],\n        [ -851.6714],\n        [ -109.0778],\n        [ -391.9370],\n        [ -155.5424],\n        [  135.5556],\n        [   34.9488],\n        [ -645.8618],\n        [ -730.0587],\n        [   84.9147],\n        [ -386.9623],\n        [ -232.5896],\n        [  171.4261],\n        [  372.5939],\n        [  137.9108],\n        [ -165.6177],\n        [ -605.4742],\n        [-1085.0206],\n        [ -420.5452],\n        [  372.2151],\n        [ -519.2092],\n        [  578.1451],\n        [  261.1541],\n        [ -340.7712],\n        [ -541.8823],\n        [ -108.1131],\n        [   88.2163],\n        [  -11.3457],\n        [  -40.3104],\n        [ -969.9188],\n        [  141.3327],\n        [ -539.0679],\n        [ -161.3901],\n        [   69.5439],\n        [ -193.5480],\n        [ -406.0835],\n        [-1024.3866],\n        [  149.4250],\n        [ -323.5292],\n        [  -43.7433],\n        [  308.4147],\n        [  138.1767],\n        [ -162.1964],\n        [ -478.6078],\n        [ -401.7161],\n        [   87.9451],\n        [ -151.7601],\n        [ -148.8831],\n        [ -749.3020],\n        [ -326.0757],\n        [  -40.6855],\n        [  -23.1055],\n        [  404.0975],\n        [  115.7222],\n        [ -471.6648],\n        [ -238.6129],\n        [ -644.7574],\n        [ -786.0811],\n        [   36.4866],\n        [ -410.9535],\n        [ -286.2466],\n        [  819.6215],\n        [ -367.3683],\n        [  375.2096],\n        [ -659.3217],\n        [  999.9402],\n        [  -85.4151],\n        [  345.4586],\n        [ -234.3533],\n        [ -560.3550],\n        [ -241.4222],\n        [ -592.2673],\n        [ -246.5307]], grad_fn=<AddBackward0>)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#实现一个多层感知器,多层感知器（Multilayer Perceptron, MLP）的隐藏层的特征数就是神经元的个数\n",
    "class MulPerceptron(nn.Module):\n",
    "    def __init__(self,input_feature,hidden_feature,out_feature):\n",
    "        nn.Module.__init__(self)\n",
    "        self.hiddenLayer=Linear(input_feature,hidden_feature)\n",
    "        self.outLayer=Linear(hidden_feature,out_feature)\n",
    "    def forward(self,x):\n",
    "        #隐藏层进行一次全连接得到（行，hidden_feature）数据矩阵\n",
    "        hout=self.hiddenLayer(x)\n",
    "        #进行relu操作\n",
    "        reOut=t.relu(hout)\n",
    "        #进行一次全连接后得到（行，out_feature）数据矩阵\n",
    "        out=self.outLayer(reOut)\n",
    "        return out;\n",
    "        \n",
    "mp=MulPerceptron(784,512,1)\n",
    "result=mp(t.randn(200,784))\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}