{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "输入数据 tensor([[[ 0.5364, -0.5291,  0.3117, -0.0282],\n         [-0.2012,  0.9933,  1.5328, -0.8234]],\n\n        [[ 1.3270, -1.2367,  0.5925,  1.0894],\n         [-1.8035,  0.3598, -0.4404,  0.4921]],\n\n        [[-0.6487, -0.0487, -0.9728,  0.7563],\n         [ 1.2929,  0.5146,  1.2296,  1.0124]]])\nRNN最后时间步隐藏层 tensor([[[0.2800, 0.8572, 0.3759],\n         [0.5901, 0.4742, 0.9417]]], grad_fn=<StackBackward>)\nRNN最后时间步隐藏层维度 torch.Size([1, 2, 3])\nRNN所有隐藏层 tensor([[[ 0.5862,  0.7417,  0.8068],\n         [ 0.9564,  0.5668,  0.6112]],\n\n        [[-0.1729,  0.7310,  0.9879],\n         [ 0.6202,  0.7824,  0.3075]],\n\n        [[ 0.2800,  0.8572,  0.3759],\n         [ 0.5901,  0.4742,  0.9417]]], grad_fn=<StackBackward>)\nRNN所有隐藏层维度 torch.Size([3, 2, 3])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "PyTorch中实现了如今最常用的三种RNN：RNN（vanilla RNN）、LSTM和GRU。此外还有对应的三种RNNCell。\n",
    "RNN和RNNCell层的区别在于前者能够处理整个序列，而后者一次只处理序列中一个时间点的数据，\n",
    "前者封装更完备更易于使用，后者更具灵活性。RNN层可以通过组合调用RNNCell来实现。\n",
    "理论参考：https://blog.csdn.net/liaomin416100569/article/details/131380370?spm=1001.2014.3001.5501\n",
    "输入参数和RNN参数解释参考readme.md\n",
    "\"\"\"\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "#注意默认（时间步，批次数，数据维度）\n",
    "sequence_length =3\n",
    "batch_size =2\n",
    "input_size =4\n",
    "input=t.randn(sequence_length,batch_size,input_size)\n",
    "print(\"输入数据\",input)\n",
    "rnnModel=nn.RNN(input_size,3,1)\n",
    "#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\n",
    "output, hidden=rnnModel(input)\n",
    "print(\"RNN最后时间步隐藏层\",hidden)\n",
    "print(\"RNN最后时间步隐藏层维度\",hidden.shape)\n",
    "print(\"RNN所有隐藏层\",output)\n",
    "print(\"RNN所有隐藏层维度\",output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "LSTM隐藏层输出的维度 torch.Size([3, 2, 3])\nLSTM隐藏层最后一个时间步输出的维度 torch.Size([1, 2, 3])\nLSTM隐藏层最后一个时间步细胞状态 torch.Size([1, 2, 3])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "lstmModel=nn.LSTM(input_size,3,1)\n",
    "#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\n",
    "output, (h, c) =lstmModel(input)\n",
    "print(\"LSTM隐藏层输出的维度\",output.shape)\n",
    "print(\"LSTM隐藏层最后一个时间步输出的维度\",h.shape)\n",
    "print(\"LSTM隐藏层最后一个时间步细胞状态\",c.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "GRU隐藏层输出的维度 torch.Size([3, 2, 3])\nGRU隐藏层最后一个时间步输出的维度 torch.Size([1, 2, 3])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# gru没有细胞状态\n",
    "gruModel=nn.GRU(input_size,3,1)\n",
    "#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\n",
    "output, h =gruModel(input)\n",
    "print(\"GRU隐藏层输出的维度\",output.shape)\n",
    "print(\"GRU隐藏层最后一个时间步输出的维度\",h.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}